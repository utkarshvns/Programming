{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import StepLR\nimport argparse","metadata":{"id":"7v8yhijkz9_r","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 6, kernel_size =5)\n        self.fc1 = nn.Linear(6*5*5, 120)\n        self.fc2 = nn.Linear(120, 84 )\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x \n\ndef convfc():\n    model_arch = Net()\n    return model_arch","metadata":{"id":"XNZBCSpC0pmY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.optim as optim\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torchvision.models as models\nimport os\nimport argparse\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\n%pip install wandb -q\nimport wandb","metadata":{"id":"s5IRMvLU1CR1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nbest_acc = 0  # best test accuracy\ndevice","metadata":{"id":"G690V8GW1rvB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('transform data')\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n])","metadata":{"id":"kARCDfSF23bQ","outputId":"a093568f-4afb-4ff3-9f87-1cd5eaf278de","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_split = .25\nrandom_seed = 1\nshuffle_dataset = True\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n\n# find data indices for training and validation splits:\ndataset_size = len(trainset)\nindices = list(range(dataset_size))\nsplit = int(np.floor(validation_split * dataset_size))\nif shuffle_dataset :\n    np.random.seed(random_seed)\n    np.random.shuffle(indices)\ntrain_indices, val_indices = indices[split:], indices[:split]\n\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalid_sampler = SubsetRandomSampler(val_indices)\n\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=128,num_workers=2,sampler=train_sampler)\n\nvalidationloader = torch.utils.data.DataLoader(trainset, batch_size=128, num_workers=2,sampler=valid_sampler)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck')","metadata":{"id":"yn-YQRkQ2_n_","outputId":"d17bddeb-68b4-4eb5-b078-349884ef4c59","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training\ndef train(epoch,net,criterion,optimizer):\n    print('\\nEpoch: %d' % epoch)\n    net.train()\n\n    running_loss = 0.0\n    val_running_loss = 0.0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)       \n        optimizer.zero_grad() # zero the parameter gradients\n\n        # forward + backward + optimize\n        outputs = net(inputs).to(device)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n    running_loss_history.append(running_loss)#for each epoch log training losses in losstrain.txt\n    \n    \n    losstrain={'epoch': [epoch],'loss':[running_loss]}## store epoch and losses dictionary\n    \n    loss_df = pd.DataFrame.from_dict(losstrain)## create dataframe from this dictionary\n    \n    loss_df.to_csv(r'train_loss.txt', header=None, index=None, sep=' ', mode='a')## dataframe to txt file\n    print('Finished Training')\n    print('Training Loss: %d' % running_loss)\n    wandb.log({\"Training Loss\": running_loss})\n\n    with torch.no_grad(): # we do not need gradient for validation.\n      for val_inputs, val_labels in validationloader:\n        val_inputs = val_inputs.to(device)\n        val_labels = val_labels.to(device)\n        val_outputs = net(val_inputs)\n        val_loss = criterion(val_outputs, val_labels)\n        \n        _, val_preds = torch.max(val_outputs, 1)\n        val_running_loss += val_loss.item()\n    val_running_loss_history.append(val_running_loss)\n    wandb.log({\"Validation Loss\": val_running_loss})","metadata":{"id":"JvVF213n3Kxw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(epoch,net,criterion):\n  global best_acc\n  net.eval()\n  total = 0\n  correct = 0\n  test_loss = 0\n  test_images = []\n  print('\\nEpoch: %d' % epoch)\n  with torch.no_grad():\n      for batch_idx, (inputs, targets) in enumerate(testloader):\n          inputs, targets = inputs.to(device), targets.to(device)\n          ## Giving input to the model\n          outputs = net(inputs)\n          loss = criterion(outputs, targets)\n          ## Predicting on the input data\n          _, predicted = torch.max(outputs.data, 1)\n          ## Defining the total size of the target\n          total += targets.size(0)\n          ## Getting the total number of correctly classified labels\n          test_loss += loss.item()\n          correct += (predicted == targets).sum().item()\n          ## Appending images to image list\n          test_images.append(wandb.Image(inputs[0], caption=\"Pred: {} Truth: {}\".format(predicted[0].item(), targets[0])))\n          ## Getting the accuracy of the model\n      model_accuracy = 100 * correct / total\n      print('Testing Accuracy: %d ' % model_accuracy)\n      print('\\n')\n      ## Checking whether the accuracy is better than the best accuracy\n      if model_accuracy>best_acc:\n      ## Saving the new value of best_accuracy\n        best_acc = model_accuracy\n        ## Saving the model\n        # Save checkpoint for the model which yields best accuracy\n        PATH = './cifar_net.pth'\n        torch.save(net.state_dict(), PATH)\n          # Write test set losses to losstest.txt for each epoch\n      df={'epoch': [epoch],'loss':[test_loss]}\n      data = pd.DataFrame.from_dict(df)\n      data.to_csv(r'losstest.txt', header=None, index=None, sep=' ', mode='a')\n      #Write test set accuracies to acctest.txt for each epoch\n      testaccuracy={'epoch': [epoch],'test_accuracy':[model_accuracy]}\n      ## Making a dataframe from this dictionary\n      accuracy_df = pd.DataFrame.from_dict(testaccuracy)\n      ## Making a txt file from this dataframe\n      accuracy_df.to_csv(r'test_accuracy.txt', header=None, index=None, sep=' ', mode='a')\n      wandb.log({\"Testing_images\": test_images,\"Test Accuracy\": 100. * correct / len(testloader.dataset),\"Test Loss\": test_loss})   \n  # Write test set losses to losstest.txt for each epoch","metadata":{"id":"8Aq5-COygscR","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training_testing(net,criterion,optimizer):\n  n_epochs_stop = 20\n  epochs_no_improve = 0\n  for epoch in range(0,500):\n    print(\"Model Training\")\n    train(epoch,net,criterion,optimizer)\n    if val_running_loss_history[epoch]>val_running_loss_history[epoch-1]:\n      if(epochs_no_improve<n_epochs_stop):\n        epochs_no_improve+=1\n      else:\n        break\n    else:\n      epochs_no_improve=0\n    print(\"\\n Model Testing\")\n    test(epoch,net,criterion)","metadata":{"id":"Fh1_cHzNKANv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_with_best_param(net):\n  ## Loading the best model parameters \n  PATH ='./cifar_net.pth'\n  net.load_state_dict(torch.load(PATH))\n  # prepare to count predictions for each class\n  correct_pred = {classname: 0 for classname in classes}\n  total_pred = {classname: 0 for classname in classes}\n\n  # again no gradients needed\n  with torch.no_grad():\n      for batch_id, (images, labels) in enumerate(testloader):\n          images, labels = images.to(device), labels.to(device)\n          outputs = net(images)\n          _, predictions = torch.max(outputs, 1)\n          # collect the correct predictions for each class\n          for label, prediction in zip(labels, predictions):\n              if label == prediction:   \n                  correct_pred[classes[label]] += 1\n              total_pred[classes[label]] += 1\n\n\n  # print accuracy for each class\n  for classname, correct_count in correct_pred.items():\n      accuracy = 100 * float(correct_count) / total_pred[classname]\n      print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n                                                    accuracy))","metadata":{"id":"exkYOhgYBHFB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def confusion_matrix_plot(net):\n  total_correct = 0\n  total_images = 0\n  confusion_matrix = np.zeros([10,10], int)\n  with torch.no_grad():\n      for batch_id, (images, labels) in enumerate(testloader):\n          images, labels = images.to(device), labels.to(device)\n          outputs = net(images)\n          _, predicted = torch.max(outputs.data, 1)\n          total_images += labels.size(0)\n          total_correct += (predicted == labels).sum().item()\n          for i, l in enumerate(labels):\n              confusion_matrix[l.item(), predicted[i].item()] += 1 \n\n  model_accuracy = total_correct / total_images * 100\n  print('Model accuracy on {0} test images: {1:.2f}%'.format(total_images, model_accuracy))\n  fig, ax = plt.subplots(1,1,figsize=(8,6))\n  sns.heatmap(confusion_matrix,annot = True,fmt='d')\n  plt.ylabel('Actual Category')\n  plt.yticks(range(10), classes)\n  plt.xlabel('Predicted Category')\n  plt.xticks(range(10), classes)\n  plt.show()","metadata":{"id":"Xm9eqIxBOPs3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ! rm -rf ./losstest.txt\n# ! rm -rf ./train_loss.txt\n# ! rm -rf ./test_accuracy.txt\n# ! rm -rf ./cifar_net.pth","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model 1","metadata":{}},{"cell_type":"code","source":"run=wandb.init(project=\"CIFAR_20BM6JP15_FADML\",reinit=True)\nwandb.watch_called = False\nnet1 = convfc()\nwandb.watch(net1,log=\"all\")\nnet1 = net1.to(device)\nrunning_loss_history = []\nval_running_loss_history = []\n### Defining a Loss function and optimizer for configuration 1\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net1.parameters(), lr=0.001, momentum=0.9)\ntraining_testing(net1,criterion,optimizer)","metadata":{"id":"1YQpkBkxLce6","outputId":"fbe43241-6ce5-44c7-b99d-0e369d20bfab","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_with_best_param(net1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix_plot(net1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Model 2","metadata":{}},{"cell_type":"code","source":"run=wandb.init(project=\"CIFAR_20BM6JP15_FADML\",reinit=True)\nwandb.watch_called = False\nnet2 = convfc()\nwandb.watch(net2,log=\"all\")\nnet2 = net2.to(device)\nrunning_loss_history = []\nval_running_loss_history = []\n### Defining a Loss function and optimizer for configuration 1\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net2.parameters(), lr=0.01)\ntraining_testing(net2,criterion,optimizer)","metadata":{"id":"1YQpkBkxLce6","outputId":"fbe43241-6ce5-44c7-b99d-0e369d20bfab","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_with_best_param(net2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix_plot(net2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model 3","metadata":{}},{"cell_type":"code","source":"run=wandb.init(project=\"CIFAR_20BM6JP15_FADML\",reinit=True)\nwandb.watch_called = False\nnet3 = convfc()\nwandb.watch(net1,log=\"all\")\nnet3 = net3.to(device)\nrunning_loss_history = []\nval_running_loss_history = []\n### Defining a Loss function and optimizer for configuration 1\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(net3.parameters(), lr=0.001, momentum=0.9)\ntraining_testing(net3,criterion,optimizer)","metadata":{"id":"1YQpkBkxLce6","outputId":"fbe43241-6ce5-44c7-b99d-0e369d20bfab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_with_best_param(net3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix_plot(net3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model 4","metadata":{}},{"cell_type":"code","source":"run=wandb.init(project=\"CIFAR_20BM6JP15_FADML\",reinit=True)\nwandb.watch_called = False\nnet4 = convfc()\nwandb.watch(net1,log=\"all\")\nnet4 = net4.to(device)\nrunning_loss_history = []\nval_running_loss_history = []\n### Defining a Loss function and optimizer for configuration 1\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(net2.parameters(), lr=0.01)\ntraining_testing(net4,criterion,optimizer)","metadata":{"id":"1YQpkBkxLce6","outputId":"fbe43241-6ce5-44c7-b99d-0e369d20bfab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_with_best_param(net4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix_plot(net4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Resnet Model","metadata":{}},{"cell_type":"code","source":"rn34 = models.resnet34(pretrained=True)\nrn34","metadata":{"id":"T67Y13FjRbg0","outputId":"f655e30e-68d3-43f3-b277-6f9ecb37a3a8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in rn34.parameters():\n  param.requires_grad = False\nrn34.fc = nn.Sequential(nn.Linear(512,256),\n                         nn.ReLU(),\n                         nn.Linear(256,128),\n                         nn.ReLU(),\n                         nn.Linear(128,10),\n                         nn.Softmax(dim =1))","metadata":{"id":"mCOTQ5-1ic4O","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run=wandb.init(project=\"CIFAR_20BM6JP15_FADML\",reinit=True)\nwandb.watch_called = False\nrn34 = rn34.to(device)\nwandb.watch(rn34,log=\"all\")\nrunning_loss_history = []\nval_running_loss_history = []\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(rn34.parameters(), lr=0.001)\ntraining_testing(rn34,criterion,optimizer)","metadata":{"id":"VQ26nD8FU-7A","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_with_best_param(rn34)","metadata":{"id":"aOMzX7qrtISf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix_plot(rn34)","metadata":{},"execution_count":null,"outputs":[]}]}