{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T05:25:15.476479Z",
     "iopub.status.busy": "2021-05-29T05:25:15.476025Z",
     "iopub.status.idle": "2021-05-29T05:25:15.485904Z",
     "shell.execute_reply": "2021-05-29T05:25:15.484831Z",
     "shell.execute_reply.started": "2021-05-29T05:25:15.476435Z"
    },
    "id": "7v8yhijkz9_r"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T05:25:16.536812Z",
     "iopub.status.busy": "2021-05-29T05:25:16.536487Z",
     "iopub.status.idle": "2021-05-29T05:25:25.093880Z",
     "shell.execute_reply": "2021-05-29T05:25:25.092930Z",
     "shell.execute_reply.started": "2021-05-29T05:25:16.536780Z"
    },
    "id": "s5IRMvLU1CR1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import argparse\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "%pip install wandb -q\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T05:25:25.096233Z",
     "iopub.status.busy": "2021-05-29T05:25:25.095874Z",
     "iopub.status.idle": "2021-05-29T05:25:25.106396Z",
     "shell.execute_reply": "2021-05-29T05:25:25.105611Z",
     "shell.execute_reply.started": "2021-05-29T05:25:25.096196Z"
    },
    "id": "XNZBCSpC0pmY"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 6, kernel_size =5)\n",
    "        self.fc1 = nn.Linear(6*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84 )\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x \n",
    "\n",
    "def convmodel():\n",
    "    model = Net()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-29T05:25:25.109745Z",
     "iopub.status.busy": "2021-05-29T05:25:25.109477Z",
     "iopub.status.idle": "2021-05-29T05:25:25.168309Z",
     "shell.execute_reply": "2021-05-29T05:25:25.167374Z",
     "shell.execute_reply.started": "2021-05-29T05:25:25.109710Z"
    },
    "id": "G690V8GW1rvB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best accuracy on test\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T21:08:41.233993Z",
     "iopub.status.busy": "2021-05-28T21:08:41.233681Z",
     "iopub.status.idle": "2021-05-28T21:08:41.240627Z",
     "shell.execute_reply": "2021-05-28T21:08:41.239619Z",
     "shell.execute_reply.started": "2021-05-28T21:08:41.233966Z"
    },
    "id": "kARCDfSF23bQ",
    "outputId": "a093568f-4afb-4ff3-9f87-1cd5eaf278de"
   },
   "outputs": [],
   "source": [
    "print('transform data')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T21:08:42.546534Z",
     "iopub.status.busy": "2021-05-28T21:08:42.546206Z",
     "iopub.status.idle": "2021-05-28T21:08:49.478277Z",
     "shell.execute_reply": "2021-05-28T21:08:49.477493Z",
     "shell.execute_reply.started": "2021-05-28T21:08:42.546505Z"
    },
    "id": "yn-YQRkQ2_n_",
    "outputId": "d17bddeb-68b4-4eb5-b078-349884ef4c59"
   },
   "outputs": [],
   "source": [
    "validation_split = .25\n",
    "random_seed = 1\n",
    "shuffle_dataset = True\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "# find data indices for training and validation splits:\n",
    "dataset_size = len(trainset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,num_workers=2,sampler=train_sampler)\n",
    "\n",
    "validationloader = torch.utils.data.DataLoader(trainset, batch_size=128, num_workers=2,sampler=valid_sampler)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T21:08:53.127646Z",
     "iopub.status.busy": "2021-05-28T21:08:53.127285Z",
     "iopub.status.idle": "2021-05-28T21:08:53.136629Z",
     "shell.execute_reply": "2021-05-28T21:08:53.13578Z",
     "shell.execute_reply.started": "2021-05-28T21:08:53.127617Z"
    },
    "id": "JvVF213n3Kxw"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch,net,criterion,optimizer):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    val_running_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)       \n",
    "        optimizer.zero_grad() # zero the parameter gradients\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs).to(device)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    running_loss_history.append(running_loss)#for each epoch log training losses in losstrain.txt\n",
    "    \n",
    "    \n",
    "    losstrain={'epoch': [epoch],'loss':[running_loss]}## store epoch and losses dictionary\n",
    "    \n",
    "    loss_df = pd.DataFrame.from_dict(losstrain)## create dataframe from this dictionary\n",
    "    \n",
    "    loss_df.to_csv(r'train_loss.txt', header=None, index=None, sep=' ', mode='a')## dataframe to txt file\n",
    "    print('Finished Training')\n",
    "    print('Training Loss: %d' % running_loss)\n",
    "    wandb.log({\"Epoch\":epoch,\"Training Loss\": running_loss})\n",
    "\n",
    "    with torch.no_grad(): # we do not need gradient for validation.\n",
    "      for val_inputs, val_labels in validationloader:\n",
    "        val_inputs = val_inputs.to(device)\n",
    "        val_labels = val_labels.to(device)\n",
    "        val_outputs = net(val_inputs)\n",
    "        val_loss = criterion(val_outputs, val_labels)\n",
    "        \n",
    "        _, val_preds = torch.max(val_outputs, 1)\n",
    "        val_running_loss += val_loss.item()\n",
    "    val_running_loss_history.append(val_running_loss)\n",
    "    wandb.log({\"Epoch\":epoch,\"Training Loss\": running_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T21:08:54.226179Z",
     "iopub.status.busy": "2021-05-28T21:08:54.225847Z",
     "iopub.status.idle": "2021-05-28T21:08:54.238756Z",
     "shell.execute_reply": "2021-05-28T21:08:54.237922Z",
     "shell.execute_reply.started": "2021-05-28T21:08:54.226148Z"
    },
    "id": "8Aq5-COygscR"
   },
   "outputs": [],
   "source": [
    "def test(epoch,net,criterion):\n",
    "  global best_acc\n",
    "  net.eval()\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  test_loss = 0\n",
    "  test_images = []\n",
    "  print('\\nEpoch: %d' % epoch)\n",
    "  with torch.no_grad():\n",
    "      for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "          inputs, targets = inputs.to(device), targets.to(device)\n",
    "          ## Giving input to the model\n",
    "          outputs = net(inputs)\n",
    "          loss = criterion(outputs, targets)\n",
    "          ## Predicting on the input data\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          ## Defining the total size of the target\n",
    "          total += targets.size(0)\n",
    "          ## Getting the total number of correctly classified labels\n",
    "          test_loss += loss.item()\n",
    "          correct += (predicted == targets).sum().item()\n",
    "          ## Appending images to image list\n",
    "          test_images.append(wandb.Image(inputs[0], caption=\"Pred: {} Truth: {}\".format(predicted[0].item(), targets[0])))\n",
    "          ## Getting the accuracy of the model\n",
    "      model_accuracy = 100 * correct / total\n",
    "      print('Testing Accuracy: %d ' % model_accuracy)\n",
    "      print('\\n')\n",
    "      ## Checking whether the accuracy is better than the best accuracy\n",
    "      if model_accuracy>best_acc:\n",
    "      ## Saving the new value of best_accuracy\n",
    "        best_acc = model_accuracy\n",
    "        ## Saving the model\n",
    "        # Save checkpoint for the model which yields best accuracy\n",
    "        PATH = './cifar_net.pth'\n",
    "        torch.save(net.state_dict(), PATH)\n",
    "          # Write test set losses to losstest.txt for each epoch\n",
    "      df={'epoch': [epoch],'loss':[test_loss]}\n",
    "      data = pd.DataFrame.from_dict(df)\n",
    "      data.to_csv(r'losstest.txt', header=None, index=None, sep=' ', mode='a')\n",
    "      #Write test set accuracies to acctest.txt for each epoch\n",
    "      testaccuracy={'epoch': [epoch],'test_accuracy':[model_accuracy]}\n",
    "      ## Making a dataframe from this dictionary\n",
    "      accuracy_df = pd.DataFrame.from_dict(testaccuracy)\n",
    "      ## Making a txt file from this dataframe\n",
    "      accuracy_df.to_csv(r'test_accuracy.txt', header=None, index=None, sep=' ', mode='a')\n",
    "      wandb.log({\"Epoch\":epoch,\"Testing_images\": test_images,\"Test Accuracy\": 100. * correct / len(testloader.dataset),\"Test Loss\": test_loss})   \n",
    "  # Write test set losses to losstest.txt for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T21:08:54.986851Z",
     "iopub.status.busy": "2021-05-28T21:08:54.986505Z",
     "iopub.status.idle": "2021-05-28T21:08:54.992274Z",
     "shell.execute_reply": "2021-05-28T21:08:54.991464Z",
     "shell.execute_reply.started": "2021-05-28T21:08:54.986822Z"
    },
    "id": "Fh1_cHzNKANv"
   },
   "outputs": [],
   "source": [
    "def training_testing(net,criterion,optimizer):\n",
    "  n_epochs_stop = 20\n",
    "  epochs_no_improve = 0\n",
    "  for epoch in range(0,500):\n",
    "    print(\"Model Training\")\n",
    "    train(epoch,net,criterion,optimizer)\n",
    "    if val_running_loss_history[epoch]>val_running_loss_history[epoch-1]:\n",
    "      if(epochs_no_improve<n_epochs_stop):\n",
    "        epochs_no_improve+=1\n",
    "      else:\n",
    "        break\n",
    "    else:\n",
    "      epochs_no_improve=0\n",
    "    print(\"\\n Model Testing\")\n",
    "    test(epoch,net,criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T21:08:56.156573Z",
     "iopub.status.busy": "2021-05-28T21:08:56.156215Z",
     "iopub.status.idle": "2021-05-28T21:08:56.165374Z",
     "shell.execute_reply": "2021-05-28T21:08:56.16444Z",
     "shell.execute_reply.started": "2021-05-28T21:08:56.15654Z"
    },
    "id": "exkYOhgYBHFB"
   },
   "outputs": [],
   "source": [
    "def best_param_prediction(net):\n",
    "  ## Loading the best model parameters \n",
    "  PATH ='./cifar_net.pth'\n",
    "  net.load_state_dict(torch.load(PATH))\n",
    "  # prepare to count predictions for each class\n",
    "  correct_pred = {classname: 0 for classname in classes}\n",
    "  total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "  # again no gradients needed\n",
    "  with torch.no_grad():\n",
    "      for batch_id, (images, labels) in enumerate(testloader):\n",
    "          images, labels = images.to(device), labels.to(device)\n",
    "          outputs = net(images)\n",
    "          _, predictions = torch.max(outputs, 1)\n",
    "          # collect the correct predictions for each class\n",
    "          for label, prediction in zip(labels, predictions):\n",
    "              if label == prediction:   \n",
    "                  correct_pred[classes[label]] += 1\n",
    "              total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "  # print accuracy for each class\n",
    "  for classname, correct_count in correct_pred.items():\n",
    "      accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "      print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                    accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T21:08:57.214454Z",
     "iopub.status.busy": "2021-05-28T21:08:57.214096Z",
     "iopub.status.idle": "2021-05-28T21:08:57.222733Z",
     "shell.execute_reply": "2021-05-28T21:08:57.221826Z",
     "shell.execute_reply.started": "2021-05-28T21:08:57.214413Z"
    },
    "id": "Xm9eqIxBOPs3"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(net):\n",
    "  total_correct = 0\n",
    "  total_images = 0\n",
    "  confusion_matrix = np.zeros([10,10], int)\n",
    "  with torch.no_grad():\n",
    "      for batch_id, (images, labels) in enumerate(testloader):\n",
    "          images, labels = images.to(device), labels.to(device)\n",
    "          outputs = net(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total_images += labels.size(0)\n",
    "          total_correct += (predicted == labels).sum().item()\n",
    "          for i, l in enumerate(labels):\n",
    "              confusion_matrix[l.item(), predicted[i].item()] += 1 \n",
    "\n",
    "  model_accuracy = total_correct / total_images * 100\n",
    "  print('Model accuracy on {0} test images: {1:.2f}%'.format(total_images, model_accuracy))\n",
    "  fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "  sns.heatmap(confusion_matrix,annot = True,fmt='d')\n",
    "  plt.ylabel('Actual Category')\n",
    "  plt.yticks(range(10), classes)\n",
    "  plt.xlabel('Predicted Category')\n",
    "  plt.xticks(range(10), classes)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T21:09:00.281635Z",
     "iopub.status.busy": "2021-05-28T21:09:00.281273Z",
     "iopub.status.idle": "2021-05-28T21:09:00.285495Z",
     "shell.execute_reply": "2021-05-28T21:09:00.284298Z",
     "shell.execute_reply.started": "2021-05-28T21:09:00.281605Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! rm -rf ./losstest.txt\n",
    "# ! rm -rf ./train_loss.txt\n",
    "# ! rm -rf ./test_accuracy.txt\n",
    "# ! rm -rf ./cifar_net.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T21:09:07.632682Z",
     "iopub.status.busy": "2021-05-28T21:09:07.632305Z"
    },
    "id": "1YQpkBkxLce6",
    "outputId": "fbe43241-6ce5-44c7-b99d-0e369d20bfab"
   },
   "outputs": [],
   "source": [
    "run=wandb.init(project=\"CIFAR_20BM6JP15_FADML_Final\",reinit=True)\n",
    "wandb.watch_called = False\n",
    "net1 = convmodel()\n",
    "wandb.watch(net1,log=\"all\")\n",
    "net1 = net1.to(device)\n",
    "running_loss_history = []\n",
    "val_running_loss_history = []\n",
    "### Defining a Loss function and optimizer for configuration 1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net1.parameters(), lr=0.001, momentum=0.9)\n",
    "training_testing(net1,criterion,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param_prediction(net1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(net1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YQpkBkxLce6",
    "outputId": "fbe43241-6ce5-44c7-b99d-0e369d20bfab"
   },
   "outputs": [],
   "source": [
    "run=wandb.init(project=\"CIFAR_20BM6JP15_FADML_Final\",reinit=True)\n",
    "wandb.watch_called = False\n",
    "net2 = convmodel()\n",
    "wandb.watch(net2,log=\"all\")\n",
    "net2 = net2.to(device)\n",
    "running_loss_history = []\n",
    "val_running_loss_history = []\n",
    "### Defining a Loss function and optimizer for configuration 1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net2.parameters(), lr=0.01)\n",
    "training_testing(net2,criterion,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param_prediction(net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YQpkBkxLce6",
    "outputId": "fbe43241-6ce5-44c7-b99d-0e369d20bfab"
   },
   "outputs": [],
   "source": [
    "run=wandb.init(project=\"CIFAR_20BM6JP15_FADML_Final\",reinit=True)\n",
    "wandb.watch_called = False\n",
    "net3 = convmodel()\n",
    "wandb.watch(net1,log=\"all\")\n",
    "net3 = net3.to(device)\n",
    "running_loss_history = []\n",
    "val_running_loss_history = []\n",
    "### Defining a Loss function and optimizer for configuration 1\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net3.parameters(), lr=0.001, momentum=0.9)\n",
    "training_testing(net3,criterion,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param_prediction(net3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(net3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YQpkBkxLce6",
    "outputId": "fbe43241-6ce5-44c7-b99d-0e369d20bfab"
   },
   "outputs": [],
   "source": [
    "run=wandb.init(project=\"CIFAR_20BM6JP15_FADML_Final\",reinit=True)\n",
    "wandb.watch_called = False\n",
    "net4 = convmodel()\n",
    "wandb.watch(net1,log=\"all\")\n",
    "net4 = net4.to(device)\n",
    "running_loss_history = []\n",
    "val_running_loss_history = []\n",
    "### Defining a Loss function and optimizer for configuration 1\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net2.parameters(), lr=0.01)\n",
    "training_testing(net4,criterion,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param_prediction(net4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(net4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T67Y13FjRbg0",
    "outputId": "f655e30e-68d3-43f3-b277-6f9ecb37a3a8"
   },
   "outputs": [],
   "source": [
    "rn34 = models.resnet34(pretrained=True)\n",
    "rn34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCOTQ5-1ic4O"
   },
   "outputs": [],
   "source": [
    "for param in rn34.parameters():\n",
    "  param.requires_grad = False\n",
    "rn34.fc = nn.Sequential(nn.Linear(512,256),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(256,128),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(128,10),\n",
    "                         nn.Softmax(dim =1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQ26nD8FU-7A"
   },
   "outputs": [],
   "source": [
    "run=wandb.init(project=\"CIFAR_20BM6JP15_FADML_Final\",reinit=True)\n",
    "wandb.watch_called = False\n",
    "rn34 = rn34.to(device)\n",
    "wandb.watch(rn34,log=\"all\")\n",
    "running_loss_history = []\n",
    "val_running_loss_history = []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rn34.parameters(), lr=0.001)\n",
    "training_testing(rn34,criterion,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOMzX7qrtISf"
   },
   "outputs": [],
   "source": [
    "best_param_prediction(rn34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(rn34)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
